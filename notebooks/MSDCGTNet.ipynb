{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a60650",
   "metadata": {},
   "source": [
    "# EEG Emotion Recognition on GAMEEMO\n",
    "\n",
    "This notebook trains a lightweight **MSDCGTNet** model on the GAMEEMO EEG dataset to classify four game‑evoked emotions:\n",
    "\n",
    "| Label | ID | Description |\n",
    "|-------|----|-------------|\n",
    "| 0 | G1 | Boring |\n",
    "| 1 | G2 | Calm |\n",
    "| 2 | G3 | Horror |\n",
    "| 3 | G4 | Funny |\n",
    "\n",
    "Run the notebook top‑to‑bottom to:\n",
    "\n",
    "1. Download and organise the data  \n",
    "2. Build subject‑wise train / validation splits  \n",
    "3. Train MSDCGTNet with early stopping & mixed precision  \n",
    "4. Inspect metrics and a confusion matrix  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82de814",
   "metadata": {},
   "source": [
    "## About the GAMEEMO dataset\n",
    "\n",
    "* **28 subjects** recorded with a 14‑channel Emotiv Epoc + headset  \n",
    "* **4 five‑minute sessions** per subject – each labelled *Boring*, *Calm*, *Horror* or *Funny*  \n",
    "* Data provided as **pre‑processed CSV** (clean, notch‑filtered, 256 Hz)\n",
    "\n",
    "The goal is to recognise the game‑elicited emotion from raw EEG.  \n",
    "We evaluate with a **subject‑wise split** to report true generalisation to unseen people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b129672",
   "metadata": {},
   "source": [
    "## 0 Setup & data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a06a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already present - skipping download.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import subprocess, sys\n",
    "\n",
    "DATA_ROOT = Path(\"../data/GAMEEMO\")\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not any(DATA_ROOT.rglob(\"*.csv\")):\n",
    "    print(\"Downloading GAMEEMO\")\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"download\",\n",
    "            \"-d\", \"sigfest/database-for-emotion-recognition-system-gameemo\",\n",
    "            \"-p\", str(DATA_ROOT.parent), \"--unzip\",\n",
    "        ], check=True)\n",
    "    except Exception as e:\n",
    "        sys.stderr.write(f\"⚠️ Kaggle download failed: {e}\\n\")\n",
    "else:\n",
    "    print(\"Dataset already present - skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b9b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 112 recordings\n",
      "Class counts: [28.0, 28.0, 28.0, 28.0] → weights: [1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path, re\n",
    "DATA_ROOT = Path(\"../data/GAMEEMO\")\n",
    "\n",
    "def g_label(fn): return int(re.search(r\"G([1-4])\", fn).group(1)) - 1\n",
    "eeg_paths, labels = [], []\n",
    "for subj in DATA_ROOT.iterdir():\n",
    "    if not subj.is_dir() or not re.fullmatch(r\"\\(S\\d{2}\\)\", subj.name): continue\n",
    "    csv_dir = subj / \"Preprocessed EEG Data\" / \".csv format\"\n",
    "    for p in csv_dir.glob(\"*.csv\"):\n",
    "        eeg_paths.append(p); labels.append(g_label(p.name))\n",
    "print(f\"Found {len(eeg_paths)} recordings\")\n",
    "\n",
    "from collections import Counter\n",
    "import torch\n",
    "cnt = Counter(labels)\n",
    "counts = torch.tensor([cnt.get(i,0) for i in range(4)], dtype=torch.float32)\n",
    "class_weights = 1.0 / counts\n",
    "class_weights = class_weights / class_weights.sum() * 4\n",
    "print(\"Class counts:\", counts.tolist(), \"→ weights:\", class_weights.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9628f",
   "metadata": {},
   "source": [
    "## 1 Dataset & DataLoader construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ac68c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: torch.Size([8, 15, 9564])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "class GameEmoEEG(Dataset):\n",
    "    \"\"\"CSV → tensor [n_channels, time]  @64 Hz (down‑sampled).\"\"\"\n",
    "    def __init__(self, paths, labels, ds=4):\n",
    "        self.p, self.y, self.ds = paths, labels, ds\n",
    "    def __len__(self):\n",
    "        return len(self.p)\n",
    "    def __getitem__(self, idx):\n",
    "        df = (\n",
    "            pd.read_csv(self.p[idx], header=None)\n",
    "              .apply(pd.to_numeric, errors='coerce')\n",
    "              .fillna(0.0)\n",
    "        )\n",
    "        x = torch.tensor(df.values.T, dtype=torch.float32)[:, :: self.ds]\n",
    "        # per‑sample z‑score normalization (channel‑wise)\n",
    "        x = (x - x.mean(dim=1, keepdim=True)) / (x.std(dim=1, keepdim=True) + 1e-6)\n",
    "        return x, torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "def _subject_from_path(p):\n",
    "    # (S01) is 4th ancestor of CSV file: .../GAMEEMO/(S01)/Preprocessed EEG Data/.csv format/file.csv\n",
    "    for part in p.parts[::-1]:\n",
    "        if part.startswith(\"(S\") and part.endswith(\")\"):\n",
    "            return part\n",
    "    raise ValueError(f\"Cannot parse subject from path: {p}\")\n",
    "\n",
    "subjects = [_subject_from_path(p) for p in eeg_paths]\n",
    "unique_subj = sorted(set(subjects))\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(unique_subj)\n",
    "val_ratio = 0.25\n",
    "n_val = max(1, int(len(unique_subj) * val_ratio))\n",
    "val_subj = set(unique_subj[:n_val])\n",
    "\n",
    "train_idx = [i for i, s in enumerate(subjects) if s not in val_subj]\n",
    "test_idx  = [i for i, s in enumerate(subjects) if s in val_subj]\n",
    "\n",
    "BATCH = 8  # little higher now that sample length is down‑sampled\n",
    "train_loader = DataLoader(\n",
    "    GameEmoEEG([eeg_paths[i] for i in train_idx], [labels[i] for i in train_idx]),\n",
    "    batch_size=BATCH, shuffle=True, drop_last=True, num_workers=0,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    GameEmoEEG([eeg_paths[i] for i in test_idx], [labels[i] for i in test_idx]),\n",
    "    batch_size=BATCH, num_workers=0,\n",
    ")\n",
    "\n",
    "sample, _ = next(iter(train_loader))\n",
    "N_CHANNELS, N_CLASSES = sample.shape[1], 4  # channels count is dim 1\n",
    "print('Sample shape:', sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd889ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 84 samples (10 batches)\n",
      "Val set size:   28 samples (4 batches)\n"
     ]
    }
   ],
   "source": [
    "## Dataset sizes\n",
    "print(f\"Train set size: {len(train_loader.dataset):,} samples ({len(train_loader):,} batches)\")\n",
    "print(f\"Val set size:   {len(test_loader.dataset):,} samples ({len(test_loader):,} batches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ed5d8",
   "metadata": {},
   "source": [
    "## 2 Model definition – MSDCGTNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2aea315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn, torch\n",
    "class MSDCGTNet(nn.Module):\n",
    "    def __init__(self, c_in, n_cls, d=64, heads=4, layers=2):\n",
    "        super().__init__()\n",
    "        self.c3 = nn.Conv1d(c_in, d, 3, 1, 1)\n",
    "        self.c5 = nn.Conv1d(c_in, d, 5, 1, 2)\n",
    "        self.c7 = nn.Conv1d(c_in, d, 7, 1, 3)\n",
    "        self.bn = nn.BatchNorm1d(3 * d)\n",
    "        self.act = nn.ReLU()\n",
    "        self.reduce = nn.AvgPool1d(4, 4)        \n",
    "        enc = nn.TransformerEncoderLayer(3 * d, heads, 4 * d, batch_first=True)\n",
    "        self.tr = nn.TransformerEncoder(enc, layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(3 * d, n_cls)\n",
    "\n",
    "    def forward(self, x):                       # x [B, C, T]\n",
    "        x = torch.cat([self.c3(x), self.c5(x), self.c7(x)], 1)\n",
    "        x = self.act(self.bn(x))\n",
    "        x = self.reduce(x)                      #  <-- sequence length ÷ 4\n",
    "        x = self.tr(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = MSDCGTNet(N_CHANNELS, N_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aaee76",
   "metadata": {},
   "source": [
    "## 3 Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed6c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep01: train 0.250, 1.461 | val 0.250, 1.416 | f1 0.144 | lr 3.0e-04\n",
      "Ep02: train 0.263, 1.396 | val 0.393, 1.366 | f1 0.267 | lr 3.0e-04\n",
      "Ep03: train 0.325, 1.391 | val 0.250, 1.347 | f1 0.100 | lr 3.0e-04\n"
     ]
    }
   ],
   "source": [
    "## 4 Training — tuned hyper‑params + EarlyStopping + F1\n",
    "import torch.nn.functional as F, torch.optim as optim, time\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import deque\n",
    "\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "CLIP_NORM = 1.0\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=3)\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience):\n",
    "        self.best = float('inf')\n",
    "        self.patience = patience\n",
    "        self.num_bad = 0\n",
    "    def step(self, metric):\n",
    "        if metric < self.best:\n",
    "            self.best = metric\n",
    "            self.num_bad = 0\n",
    "            return False  # keep going\n",
    "        else:\n",
    "            self.num_bad += 1\n",
    "            return self.num_bad > self.patience\n",
    "\n",
    "stopper = EarlyStopper(PATIENCE)\n",
    "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())  # mixed precision\n",
    "\n",
    "def run(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    tot = correct = loss_sum = 0.0\n",
    "    all_pred, all_true = [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            with torch.amp.autocast(enabled=torch.cuda.is_available(), device_type=device.type):\n",
    "                out = model(x)\n",
    "                loss = F.cross_entropy(out, y, weight=class_weights.to(device))\n",
    "            if train:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "        pred = out.argmax(1)\n",
    "        all_pred.extend(pred.cpu().numpy())\n",
    "        all_true.extend(y.cpu().numpy())\n",
    "        correct += (pred == y).sum().item()\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "        tot += y.size(0)\n",
    "    acc = correct / tot\n",
    "    f1 = f1_score(all_true, all_pred, average='macro')\n",
    "    return acc, loss_sum / tot, f1\n",
    "\n",
    "start = time.time()\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    tr_a, tr_l, tr_f1 = run(train_loader, True)\n",
    "    v_a, v_l, v_f1 = run(test_loader, False)\n",
    "    scheduler.step(v_l)\n",
    "    lr_now = opt.param_groups[0]['lr']\n",
    "    print(f\"Ep{ep:02d}: train {tr_a:.3f}, {tr_l:.3f} | val {v_a:.3f}, {v_l:.3f} | f1 {v_f1:.3f} | lr {lr_now:.1e}\")\n",
    "    if stopper.step(v_l):\n",
    "        print(\"Early stopping❗\")\n",
    "        break\n",
    "print(\"Total:\", round(time.time() - start, 1), \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f7232",
   "metadata": {},
   "source": [
    "## 4 Evaluation & confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ccf141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "model.eval(); y_t,y_p=[],[]\n",
    "with torch.no_grad():\n",
    "    for x,y in test_loader:\n",
    "        y_p.extend(model(x.to(device)).argmax(1).cpu().numpy())\n",
    "        y_t.extend(y.numpy())\n",
    "classes=['Boring','Calm','Horror','Funny']\n",
    "print(classification_report(y_t,y_p,target_names=classes))\n",
    "cm=confusion_matrix(y_t,y_p)\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "ax.imshow(cm,cmap='Blues')\n",
    "ax.set_xticks(range(4)); ax.set_yticks(range(4))\n",
    "ax.set_xticklabels(classes); ax.set_yticklabels(classes)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax.text(j,i,cm[i,j],ha='center',va='center',\n",
    "                color='white' if cm[i,j]>cm.max()/2 else 'black')\n",
    "ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc46ea",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- **MSDCGTNet**: Multi-Scale Dynamic 1D CNN + Gated Transformer network for EEG emotion recognition, combining spatial-spectral feature extraction with long-range dependency modeling.  \n",
    "- **Vaswani et al., 2017**: “Attention Is All You Need,” in *Advances in Neural Information Processing Systems*.  \n",
    "- **Türkan et al., 2020**: “EEG Signals and Various Computer Games (GAMEEMO),” *Biomedical Signal Processing and Control*, DOI:10.1016/j.bspc.2020.101951.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
