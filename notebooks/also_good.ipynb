{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a60650",
   "metadata": {},
   "source": [
    "# GAMEEMO × MSDCGTNet — **Stable & Fast**\n",
    "• correct `[64 channels, time]` layout\n",
    "• down‑sampled to 64 Hz\n",
    "• single‑process DataLoader (Windows‑safe)\n",
    "• batch 8, epochs finish in a couple of minutes on a GTX 2060\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82de814",
   "metadata": {},
   "source": [
    "**Hot‑fix – 2025-05-14**\n",
    "\n",
    "‑ Fixed `N_CHANNELS` mis‑assignment (now uses `sample.shape[1]`).\n",
    "‑ Updated AMP API calls (deprecated `torch.cuda.amp.*` → `torch.amp.*`).\n",
    "\n",
    "Run all cells from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b129672",
   "metadata": {},
   "source": [
    "## 1 Data discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a06a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already present - skipping download.\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "from pathlib import Path\n",
    "import subprocess, sys\n",
    "\n",
    "DATA_ROOT = Path(\"../data/GAMEEMO\")\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not any(DATA_ROOT.rglob(\"*.csv\")):\n",
    "    print(\"Downloading GAMEEMO\")\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"kaggle\", \"datasets\", \"download\",\n",
    "            \"-d\", \"sigfest/database-for-emotion-recognition-system-gameemo\",\n",
    "            \"-p\", str(DATA_ROOT.parent), \"--unzip\",\n",
    "        ], check=True)\n",
    "    except Exception as e:\n",
    "        sys.stderr.write(f\"⚠️ Kaggle download failed: {e}\\n\")\n",
    "else:\n",
    "    print(\"Dataset already present - skipping download.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b9b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 112 recordings\n",
      "Class counts: [28.0, 28.0, 28.0, 28.0] → weights: [1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path, re\n",
    "import torch\n",
    "\n",
    "DATA_ROOT = Path(\"../data/GAMEEMO\")\n",
    "\n",
    "def g_label(fn): return int(re.search(r\"G([1-4])\", fn).group(1)) - 1\n",
    "eeg_paths, labels = [], []\n",
    "for subj in DATA_ROOT.iterdir():\n",
    "    if not subj.is_dir() or not re.fullmatch(r\"\\(S\\d{2}\\)\", subj.name): continue\n",
    "    csv_dir = subj / \"Preprocessed EEG Data\" / \".csv format\"\n",
    "    for p in csv_dir.glob(\"*.csv\"):\n",
    "        eeg_paths.append(p); labels.append(g_label(p.name))\n",
    "print(f\"Found {len(eeg_paths)} recordings\")\n",
    "\n",
    "# --- class weights for imbalanced data ---\n",
    "from collections import Counter\n",
    "cnt = Counter(labels)\n",
    "counts = torch.tensor([cnt.get(i,0) for i in range(4)], dtype=torch.float32)\n",
    "class_weights = 1.0 / counts\n",
    "class_weights = class_weights / class_weights.sum() * 4\n",
    "print(\"Class counts:\", counts.tolist(), \"→ weights:\", class_weights.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9628f",
   "metadata": {},
   "source": [
    "## 2 Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ac68c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: torch.Size([8, 15, 9564])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "class GameEmoEEG(Dataset):\n",
    "    \"\"\"CSV → tensor [n_channels, time]  @64 Hz (down‑sampled).\"\"\"\n",
    "    def __init__(self, paths, labels, ds=4):\n",
    "        self.p, self.y, self.ds = paths, labels, ds\n",
    "    def __len__(self):\n",
    "        return len(self.p)\n",
    "    def __getitem__(self, idx):\n",
    "        df = (\n",
    "            pd.read_csv(self.p[idx], header=None)\n",
    "              .apply(pd.to_numeric, errors='coerce')\n",
    "              .fillna(0.0)\n",
    "        )\n",
    "        x = torch.tensor(df.values.T, dtype=torch.float32)[:, :: self.ds]\n",
    "        # per‑sample z‑score normalization (channel‑wise)\n",
    "        x = (x - x.mean(dim=1, keepdim=True)) / (x.std(dim=1, keepdim=True) + 1e-6)\n",
    "        return x, torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "# --- subject‑wise split to avoid leakage ---\n",
    "def _subject_from_path(p):\n",
    "    # (S01) is 4th ancestor of CSV file: .../GAMEEMO/(S01)/Preprocessed EEG Data/.csv format/file.csv\n",
    "    for part in p.parts[::-1]:\n",
    "        if part.startswith(\"(S\") and part.endswith(\")\"):\n",
    "            return part\n",
    "    raise ValueError(f\"Cannot parse subject from path: {p}\")\n",
    "\n",
    "subjects = [_subject_from_path(p) for p in eeg_paths]\n",
    "unique_subj = sorted(set(subjects))\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(unique_subj)\n",
    "val_ratio = 0.2\n",
    "n_val = max(1, int(len(unique_subj) * val_ratio))\n",
    "val_subj = set(unique_subj[:n_val])\n",
    "\n",
    "train_idx = [i for i, s in enumerate(subjects) if s not in val_subj]\n",
    "test_idx  = [i for i, s in enumerate(subjects) if s in val_subj]\n",
    "\n",
    "BATCH = 8  # little higher now that sample length is down‑sampled\n",
    "train_loader = DataLoader(\n",
    "    GameEmoEEG([eeg_paths[i] for i in train_idx], [labels[i] for i in train_idx]),\n",
    "    batch_size=BATCH, shuffle=True, drop_last=True, num_workers=0,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    GameEmoEEG([eeg_paths[i] for i in test_idx], [labels[i] for i in test_idx]),\n",
    "    batch_size=BATCH, num_workers=0,\n",
    ")\n",
    "\n",
    "sample, _ = next(iter(train_loader))\n",
    "N_CHANNELS, N_CLASSES = sample.shape[1], 4  # channels count is dim 1\n",
    "print('Sample shape:', sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd889ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 92 samples (11 batches)\n",
      "Val set size:   20 samples (3 batches)\n"
     ]
    }
   ],
   "source": [
    "## Dataset sizes\n",
    "print(f\"Train set size: {len(train_loader.dataset):,} samples ({len(train_loader):,} batches)\")\n",
    "print(f\"Val set size:   {len(test_loader.dataset):,} samples ({len(test_loader):,} batches)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ed5d8",
   "metadata": {},
   "source": [
    "## 3 Model — MSDCGTNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2aea315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn, torch\n",
    "class MSDCGTNet(nn.Module):\n",
    "    def __init__(self, c_in, n_cls, d=64, heads=4, layers=2):\n",
    "        super().__init__()\n",
    "        self.c3 = nn.Conv1d(c_in, d, 3, 1, 1)\n",
    "        self.c5 = nn.Conv1d(c_in, d, 5, 1, 2)\n",
    "        self.c7 = nn.Conv1d(c_in, d, 7, 1, 3)\n",
    "        self.bn = nn.BatchNorm1d(3 * d)\n",
    "        self.act = nn.ReLU()\n",
    "        self.reduce = nn.AvgPool1d(4, 4)        # ← NEW: 4× temporal shrink\n",
    "        enc = nn.TransformerEncoderLayer(3 * d, heads, 4 * d, batch_first=True)\n",
    "        self.tr = nn.TransformerEncoder(enc, layers)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(3 * d, n_cls)\n",
    "\n",
    "    def forward(self, x):                       # x [B, C, T]\n",
    "        x = torch.cat([self.c3(x), self.c5(x), self.c7(x)], 1)\n",
    "        x = self.act(self.bn(x))\n",
    "        x = self.reduce(x)                      #  <-- sequence length ÷ 4\n",
    "        x = self.tr(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model  = MSDCGTNet(N_CHANNELS, N_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aaee76",
   "metadata": {},
   "source": [
    "## 4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eed6c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep01: train 0.250 | val 0.250 | f1 0.100 | lr 3.00e-04\n",
      "Ep02: train 0.205 | val 0.250 | f1 0.100 | lr 3.00e-04\n",
      "Ep03: train 0.284 | val 0.250 | f1 0.100 | lr 3.00e-04\n",
      "Ep04: train 0.284 | val 0.300 | f1 0.188 | lr 3.00e-04\n",
      "Ep05: train 0.352 | val 0.550 | f1 0.476 | lr 3.00e-04\n",
      "Ep06: train 0.466 | val 0.300 | f1 0.188 | lr 3.00e-04\n",
      "Ep07: train 0.409 | val 0.450 | f1 0.393 | lr 3.00e-04\n",
      "Ep08: train 0.443 | val 0.450 | f1 0.306 | lr 3.00e-04\n",
      "Ep09: train 0.420 | val 0.450 | f1 0.390 | lr 3.00e-04\n",
      "Ep10: train 0.636 | val 0.600 | f1 0.583 | lr 3.00e-04\n",
      "Ep11: train 0.682 | val 0.750 | f1 0.753 | lr 3.00e-04\n",
      "Ep12: train 0.716 | val 0.750 | f1 0.755 | lr 3.00e-04\n",
      "Ep13: train 0.898 | val 0.900 | f1 0.899 | lr 3.00e-04\n",
      "Ep14: train 0.909 | val 0.800 | f1 0.796 | lr 3.00e-04\n",
      "Ep15: train 0.932 | val 0.800 | f1 0.798 | lr 3.00e-04\n",
      "Ep16: train 0.932 | val 0.800 | f1 0.802 | lr 3.00e-04\n",
      "Ep17: train 0.943 | val 0.800 | f1 0.796 | lr 3.00e-04\n",
      "Ep18: train 0.966 | val 0.850 | f1 0.850 | lr 3.00e-04\n",
      "Ep19: train 0.977 | val 0.900 | f1 0.899 | lr 3.00e-04\n",
      "Ep20: train 0.977 | val 0.900 | f1 0.899 | lr 3.00e-04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m start = time.time()\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     tr_a, tr_l, tr_f1 = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     v_a, v_l, v_f1 = run(test_loader, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     62\u001b[39m     scheduler.step(v_l)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(loader, train)\u001b[39m\n\u001b[32m     34\u001b[39m tot = correct = loss_sum = \u001b[32m0.0\u001b[39m\n\u001b[32m     35\u001b[39m all_pred, all_true = [], []\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_grad_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dmrom\\emotion-recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dmrom\\emotion-recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dmrom\\emotion-recognition\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mGameEmoEEG.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m     12\u001b[39m     df = (\n\u001b[32m     13\u001b[39m         \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43m          \u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcoerce\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m           .fillna(\u001b[32m0.0\u001b[39m)\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     17\u001b[39m     x = torch.tensor(df.values.T, dtype=torch.float32)[:, :: \u001b[38;5;28mself\u001b[39m.ds]\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# per‑sample z‑score normalization (channel‑wise)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dmrom\\emotion-recognition\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[39m, in \u001b[36mDataFrame.apply\u001b[39m\u001b[34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m  10360\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[32m  10362\u001b[39m op = frame_apply(\n\u001b[32m  10363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  10364\u001b[39m     func=func,\n\u001b[32m   (...)\u001b[39m\u001b[32m  10372\u001b[39m     kwargs=kwargs,\n\u001b[32m  10373\u001b[39m )\n\u001b[32m> \u001b[39m\u001b[32m10374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dmrom\\emotion-recognition\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[39m, in \u001b[36mFrameApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_raw(engine=\u001b[38;5;28mself\u001b[39m.engine, engine_kwargs=\u001b[38;5;28mself\u001b[39m.engine_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dmrom\\emotion-recognition\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[39m, in \u001b[36mFrameApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1062\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m         results, res_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m         results, res_index = \u001b[38;5;28mself\u001b[39m.apply_series_numba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dmrom\\emotion-recognition\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[39m, in \u001b[36mFrameApply.apply_series_generator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1078\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1079\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[32m   1080\u001b[39m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m         results[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1082\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[32m   1083\u001b[39m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[32m   1084\u001b[39m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[32m   1085\u001b[39m             results[i] = results[i].copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dmrom\\emotion-recognition\\.venv\\Lib\\site-packages\\pandas\\core\\tools\\numeric.py:319\u001b[39m, in \u001b[36mto_numeric\u001b[39m\u001b[34m(arg, errors, downcast, dtype_backend)\u001b[39m\n\u001b[32m    316\u001b[39m         values = ArrowExtensionArray(values.__arrow_array__())\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_series:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_constructor\u001b[49m(values, index=arg.index, name=arg.name)\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_index:\n\u001b[32m    321\u001b[39m     \u001b[38;5;66;03m# because we want to coerce to numeric if possible,\u001b[39;00m\n\u001b[32m    322\u001b[39m     \u001b[38;5;66;03m# do not use _shallow_copy\u001b[39;00m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Index\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dmrom\\emotion-recognition\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:660\u001b[39m, in \u001b[36mSeries._constructor\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m s._mgr, s.index\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_constructor\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Callable[..., Series]:\n\u001b[32m    662\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Series\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_constructor_from_mgr\u001b[39m(\u001b[38;5;28mself\u001b[39m, mgr, axes):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## 4 Training — tuned hyper‑params + EarlyStopping + F1\n",
    "import torch.nn.functional as F, torch.optim as optim, time\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import deque\n",
    "\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-3\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "CLIP_NORM = 1.0\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=3)\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience):\n",
    "        self.best = float('inf')\n",
    "        self.patience = patience\n",
    "        self.num_bad = 0\n",
    "    def step(self, metric):\n",
    "        if metric < self.best:\n",
    "            self.best = metric\n",
    "            self.num_bad = 0\n",
    "            return False  # keep going\n",
    "        else:\n",
    "            self.num_bad += 1\n",
    "            return self.num_bad > self.patience\n",
    "\n",
    "stopper = EarlyStopper(PATIENCE)\n",
    "scaler = torch.amp.GradScaler(enabled=torch.cuda.is_available())  # mixed precision\n",
    "\n",
    "def run(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    tot = correct = loss_sum = 0.0\n",
    "    all_pred, all_true = [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            with torch.amp.autocast(enabled=torch.cuda.is_available(), device_type=device.type):\n",
    "                out = model(x)\n",
    "                loss = F.cross_entropy(out, y, weight=class_weights.to(device))\n",
    "            if train:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(opt)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "        pred = out.argmax(1)\n",
    "        all_pred.extend(pred.cpu().numpy())\n",
    "        all_true.extend(y.cpu().numpy())\n",
    "        correct += (pred == y).sum().item()\n",
    "        loss_sum += loss.item() * y.size(0)\n",
    "        tot += y.size(0)\n",
    "    acc = correct / tot\n",
    "    f1 = f1_score(all_true, all_pred, average='macro')\n",
    "    return acc, loss_sum / tot, f1\n",
    "\n",
    "start = time.time()\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    tr_a, tr_l, tr_f1 = run(train_loader, True)\n",
    "    v_a, v_l, v_f1 = run(test_loader, False)\n",
    "    scheduler.step(v_l)\n",
    "    lr_now = opt.param_groups[0]['lr']\n",
    "    print(f\"Ep{ep:02d}: train {tr_a:.3f} | val {v_a:.3f} | f1 {v_f1:.3f} | lr {lr_now:.2e}\")\n",
    "    if stopper.step(v_l):\n",
    "        print(\"Early stopping❗\")\n",
    "        break\n",
    "print(\"Total:\", round(time.time() - start, 1), \"s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f7232",
   "metadata": {},
   "source": [
    "## 5 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ccf141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Boring       0.83      1.00      0.91         5\n",
      "        Calm       0.80      0.80      0.80         5\n",
      "      Horror       1.00      0.80      0.89         5\n",
      "       Funny       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.90        20\n",
      "   macro avg       0.91      0.90      0.90        20\n",
      "weighted avg       0.91      0.90      0.90        20\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "model.eval(); y_t,y_p=[],[]\n",
    "with torch.no_grad():\n",
    "    for x,y in test_loader:\n",
    "        y_p.extend(model(x.to(device)).argmax(1).cpu().numpy())\n",
    "        y_t.extend(y.numpy())\n",
    "classes=['Boring','Calm','Horror','Funny']\n",
    "print(classification_report(y_t,y_p,target_names=classes))\n",
    "cm=confusion_matrix(y_t,y_p)\n",
    "fig,ax=plt.subplots(figsize=(4,4))\n",
    "ax.imshow(cm,cmap='Blues')\n",
    "ax.set_xticks(range(4)); ax.set_yticks(range(4))\n",
    "ax.set_xticklabels(classes); ax.set_yticklabels(classes)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        ax.text(j,i,cm[i,j],ha='center',va='center',\n",
    "                color='white' if cm[i,j]>cm.max()/2 else 'black')\n",
    "ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc46ea",
   "metadata": {},
   "source": [
    "## References\n",
    "- MSDCGTNet (Cheng et al., 2024)\n",
    "- GAMEEMO dataset on Kaggle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
